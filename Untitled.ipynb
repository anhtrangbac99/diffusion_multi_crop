{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521dffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as TF\n",
    "import torchvision.datasets as dsets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "# import matplotlib.pyplot as plt\n",
    "import utils\n",
    "from guided_diffusion.unet import UNetModel\n",
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import json\n",
    "from collections import namedtuple\n",
    "import argparse\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from blur_diffusion import Deblurring, ForwardBlurIncreasing, gaussian_kernel_1d\n",
    "from utils import normalize_np, clear\n",
    "from EMA import EMA\n",
    "from torch.nn import DataParallel\n",
    "from fid import FID\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e660bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Configs')\n",
    "parser.add_argument('--gpu', default='0',type=str, help='gpu num')\n",
    "parser.add_argument('--dataset',default='cifar10', type=str, help='cifar10 / mnist')\n",
    "parser.add_argument('--name', default='blur_diff',type=str, help='Saving directory name')\n",
    "parser.add_argument('--ckpt', default='', type=str, help='UNet checkpoint')\n",
    "\n",
    "parser.add_argument('--bsize', default=8, type=int, help='batchsize')\n",
    "parser.add_argument('--N', default=500, type=int, help='Max diffusion timesteps')\n",
    "parser.add_argument('--sig', default=0.4, type=float, help='sigma value for blur kernel')\n",
    "parser.add_argument('--sig_min', default=0, type=float, help='sigma value for blur kernel')\n",
    "parser.add_argument('--sig_max', default=0.1, type=float, help='sigma value for blur kernel')\n",
    "parser.add_argument('--lr', default=0.00005, type=float, help='learning rate')\n",
    "parser.add_argument('--noise_schedule', default='linear', type=str, help='Type of noise schedule to use')\n",
    "parser.add_argument('--betamin', default=0.0001, type=float, help='beta (min). get_score(1) can diverge if this is too low.')\n",
    "parser.add_argument('--betamax', default=0.02, type=float, help='beta (max)')\n",
    "parser.add_argument('--fromprior', default=True, type=bool, help='start sampling from prior')\n",
    "parser.add_argument('--gtscore', action='store_true', help='Use ground truth score for reverse diffusion')\n",
    "parser.add_argument('--max_iter', default=15000, type=int, help='max iterations')\n",
    "parser.add_argument('--eval_iter', default=1000, type=int, help='eval iterations')\n",
    "parser.add_argument('--fid_iter', default=2000, type=int, help='eval iterations')\n",
    "parser.add_argument('--fid_num_samples', default=100, type=int, help='eval iterations')\n",
    "parser.add_argument('--fid_bsize', default=8, type=int, help='eval iterations')\n",
    "parser.add_argument('--loss_type', type=str, default = 'eps_simple', choices=['sm_simple', 'eps_simple', 'sm_exact', 'std_matching'])\n",
    "parser.add_argument('--f_type', type=str, default = 'linear', choices=['linear', 'log', 'quadratic', 'cubic', 'quartic', 'triangular'])\n",
    "parser.add_argument('--dropout', default=0, type=float, help='dropout')\n",
    "\n",
    "# EMA, save\n",
    "parser.add_argument('--use_ema', action='store_true',\n",
    "                    help='use EMA or not')\n",
    "parser.add_argument('--inference', action='store_true')\n",
    "parser.add_argument('--freq_feat', action='store_true', help = \"concat Utx_i\")\n",
    "parser.add_argument('--ode', action='store_true', help = \"ODE fast sampler\")\n",
    "parser.add_argument('--ema_decay', type=float, default=0.9999, help='decay rate for EMA')\n",
    "parser.add_argument('--save_every', type=int, default=5000, help='How often we wish to save ckpts')\n",
    "opt = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a7e20377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 500\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f'cuda:{opt.gpu}')\n",
    "device = torch.device('cuda')\n",
    "print(\"N:\", opt.N)\n",
    "N = opt.N\n",
    "bsize = opt.bsize\n",
    "beta_min = opt.betamin\n",
    "beta_max = opt.betamax\n",
    "sig = opt.sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae3ab4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transformer =  transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(0.5), \n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9124416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.CIFAR10(root='.', train=True, transform=train_transformer, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='.', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b8f7f945",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=opt.bsize,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=opt.bsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d71d3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('experiments','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b6fc182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fid_eval = FID(real_dir =train_dir, device = device,bsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bea7bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains zero? tensor(False, device='cuda:0')\n",
      "blur.U_small.shape: torch.Size([32, 32])\n",
      "fs:  tensor([-0.0001,  0.0000,  0.0001,  0.0003,  0.0004,  0.0005,  0.0006,  0.0008,\n",
      "         0.0009,  0.0010,  0.0011,  0.0013,  0.0014,  0.0015,  0.0016,  0.0018,\n",
      "         0.0019,  0.0020,  0.0021,  0.0023,  0.0024,  0.0025,  0.0026,  0.0028,\n",
      "         0.0029,  0.0030,  0.0031,  0.0033,  0.0034,  0.0035,  0.0036,  0.0038,\n",
      "         0.0039,  0.0040,  0.0041,  0.0043,  0.0044,  0.0045,  0.0046,  0.0048,\n",
      "         0.0049,  0.0050,  0.0051,  0.0053,  0.0054,  0.0055,  0.0056,  0.0058,\n",
      "         0.0059,  0.0060,  0.0061,  0.0063,  0.0064,  0.0065,  0.0066,  0.0068,\n",
      "         0.0069,  0.0070,  0.0071,  0.0073,  0.0074,  0.0075,  0.0076,  0.0078,\n",
      "         0.0079,  0.0080,  0.0081,  0.0083,  0.0084,  0.0085,  0.0086,  0.0088,\n",
      "         0.0089,  0.0090,  0.0091,  0.0093,  0.0094,  0.0095,  0.0096,  0.0098,\n",
      "         0.0099,  0.0100,  0.0101,  0.0103,  0.0104,  0.0105,  0.0106,  0.0108,\n",
      "         0.0109,  0.0110,  0.0111,  0.0113,  0.0114,  0.0115,  0.0116,  0.0118,\n",
      "         0.0119,  0.0120,  0.0121,  0.0123,  0.0124,  0.0125,  0.0127,  0.0128,\n",
      "         0.0129,  0.0130,  0.0132,  0.0133,  0.0134,  0.0135,  0.0137,  0.0138,\n",
      "         0.0139,  0.0140,  0.0142,  0.0143,  0.0144,  0.0145,  0.0147,  0.0148,\n",
      "         0.0149,  0.0150,  0.0152,  0.0153,  0.0154,  0.0155,  0.0157,  0.0158,\n",
      "         0.0159,  0.0160,  0.0162,  0.0163,  0.0164,  0.0165,  0.0167,  0.0168,\n",
      "         0.0169,  0.0170,  0.0172,  0.0173,  0.0174,  0.0175,  0.0177,  0.0178,\n",
      "         0.0179,  0.0180,  0.0182,  0.0183,  0.0184,  0.0185,  0.0187,  0.0188,\n",
      "         0.0189,  0.0190,  0.0192,  0.0193,  0.0194,  0.0195,  0.0197,  0.0198,\n",
      "         0.0199,  0.0200,  0.0202,  0.0203,  0.0204,  0.0205,  0.0207,  0.0208,\n",
      "         0.0209,  0.0210,  0.0212,  0.0213,  0.0214,  0.0215,  0.0217,  0.0218,\n",
      "         0.0219,  0.0220,  0.0222,  0.0223,  0.0224,  0.0225,  0.0227,  0.0228,\n",
      "         0.0229,  0.0230,  0.0232,  0.0233,  0.0234,  0.0235,  0.0237,  0.0238,\n",
      "         0.0239,  0.0240,  0.0242,  0.0243,  0.0244,  0.0245,  0.0247,  0.0248,\n",
      "         0.0249,  0.0251,  0.0252,  0.0253,  0.0254,  0.0256,  0.0257,  0.0258,\n",
      "         0.0259,  0.0261,  0.0262,  0.0263,  0.0264,  0.0266,  0.0267,  0.0268,\n",
      "         0.0269,  0.0271,  0.0272,  0.0273,  0.0274,  0.0276,  0.0277,  0.0278,\n",
      "         0.0279,  0.0281,  0.0282,  0.0283,  0.0284,  0.0286,  0.0287,  0.0288,\n",
      "         0.0289,  0.0291,  0.0292,  0.0293,  0.0294,  0.0296,  0.0297,  0.0298,\n",
      "         0.0299,  0.0301,  0.0302,  0.0303,  0.0304,  0.0306,  0.0307,  0.0308,\n",
      "         0.0309,  0.0311,  0.0312,  0.0313,  0.0314,  0.0316,  0.0317,  0.0318,\n",
      "         0.0319,  0.0321,  0.0322,  0.0323,  0.0324,  0.0326,  0.0327,  0.0328,\n",
      "         0.0329,  0.0331,  0.0332,  0.0333,  0.0334,  0.0336,  0.0337,  0.0338,\n",
      "         0.0339,  0.0341,  0.0342,  0.0343,  0.0344,  0.0346,  0.0347,  0.0348,\n",
      "         0.0349,  0.0351,  0.0352,  0.0353,  0.0354,  0.0356,  0.0357,  0.0358,\n",
      "         0.0359,  0.0361,  0.0362,  0.0363,  0.0364,  0.0366,  0.0367,  0.0368,\n",
      "         0.0369,  0.0371,  0.0372,  0.0373,  0.0374,  0.0376,  0.0377,  0.0378,\n",
      "         0.0380,  0.0381,  0.0382,  0.0383,  0.0385,  0.0386,  0.0387,  0.0388,\n",
      "         0.0390,  0.0391,  0.0392,  0.0393,  0.0395,  0.0396,  0.0397,  0.0398,\n",
      "         0.0400,  0.0401,  0.0402,  0.0403,  0.0405,  0.0406,  0.0407,  0.0408,\n",
      "         0.0410,  0.0411,  0.0412,  0.0413,  0.0415,  0.0416,  0.0417,  0.0418,\n",
      "         0.0420,  0.0421,  0.0422,  0.0423,  0.0425,  0.0426,  0.0427,  0.0428,\n",
      "         0.0430,  0.0431,  0.0432,  0.0433,  0.0435,  0.0436,  0.0437,  0.0438,\n",
      "         0.0440,  0.0441,  0.0442,  0.0443,  0.0445,  0.0446,  0.0447,  0.0448,\n",
      "         0.0450,  0.0451,  0.0452,  0.0453,  0.0455,  0.0456,  0.0457,  0.0458,\n",
      "         0.0460,  0.0461,  0.0462,  0.0463,  0.0465,  0.0466,  0.0467,  0.0468,\n",
      "         0.0470,  0.0471,  0.0472,  0.0473,  0.0475,  0.0476,  0.0477,  0.0478,\n",
      "         0.0480,  0.0481,  0.0482,  0.0483,  0.0485,  0.0486,  0.0487,  0.0488,\n",
      "         0.0490,  0.0491,  0.0492,  0.0493,  0.0495,  0.0496,  0.0497,  0.0498,\n",
      "         0.0500,  0.0501,  0.0502,  0.0504,  0.0505,  0.0506,  0.0507,  0.0509,\n",
      "         0.0510,  0.0511,  0.0512,  0.0514,  0.0515,  0.0516,  0.0517,  0.0519,\n",
      "         0.0520,  0.0521,  0.0522,  0.0524,  0.0525,  0.0526,  0.0527,  0.0529,\n",
      "         0.0530,  0.0531,  0.0532,  0.0534,  0.0535,  0.0536,  0.0537,  0.0539,\n",
      "         0.0540,  0.0541,  0.0542,  0.0544,  0.0545,  0.0546,  0.0547,  0.0549,\n",
      "         0.0550,  0.0551,  0.0552,  0.0554,  0.0555,  0.0556,  0.0557,  0.0559,\n",
      "         0.0560,  0.0561,  0.0562,  0.0564,  0.0565,  0.0566,  0.0567,  0.0569,\n",
      "         0.0570,  0.0571,  0.0572,  0.0574,  0.0575,  0.0576,  0.0577,  0.0579,\n",
      "         0.0580,  0.0581,  0.0582,  0.0584,  0.0585,  0.0586,  0.0587,  0.0589,\n",
      "         0.0590,  0.0591,  0.0592,  0.0594,  0.0595,  0.0596,  0.0597,  0.0599,\n",
      "         0.0600,  0.0601,  0.0602,  0.0604,  0.0605,  0.0606,  0.0607,  0.0609,\n",
      "         0.0610,  0.0611,  0.0612,  0.0614,  0.0615,  0.0616,  0.0617,  0.0619,\n",
      "         0.0620,  0.0621,  0.0622,  0.0624,  0.0625], device='cuda:0')\n",
      "p:  torch.Size([501, 3072])\n",
      "D:  torch.Size([501, 3072])\n"
     ]
    }
   ],
   "source": [
    "resolution = train_dataset[0][0].shape[-1]\n",
    "input_nc = train_dataset[0][0].shape[0]\n",
    "ksize = resolution * 2 - 1\n",
    "pad = 0\n",
    "\n",
    "# define forward blur\n",
    "kernel = gaussian_kernel_1d(ksize, sig)\n",
    "blur = Deblurring(kernel, input_nc, resolution, device=device)\n",
    "print(\"blur.U_small.shape:\", blur.U_small.shape)\n",
    "D_diag = blur.singulars()\n",
    "fb = ForwardBlurIncreasing(N=N, beta_min=beta_min, beta_max=beta_max, sig=sig, sig_max = opt.sig_max, sig_min = opt.sig_min, D_diag=D_diag,\n",
    "                    blur=blur, channel=input_nc, device=device, noise_schedule=opt.noise_schedule, resolution=resolution, pad=pad, f_type=opt.f_type)\n",
    "dir = os.path.join('experiments', opt.name)\n",
    "writer = SummaryWriter(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d600cb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_blocks torch.Size([128, 3, 3, 3])\n",
      "Let's use 3 GPUs!\n",
      "input_nc 3 resolution 32\n",
      "MAE = 3.856495368381729e-06\n",
      "x_50.std() = 0.3895820379257202\n",
      "x_50.mean() = 0.4998919367790222\n",
      "x_100.std() = 0.6231688261032104\n",
      "x_100.mean() = 0.4730055332183838\n",
      "x_150.std() = 0.7919426560401917\n",
      "x_150.mean() = 0.3916463553905487\n",
      "x_200.std() = 0.9058626294136047\n",
      "x_200.mean() = 0.32252490520477295\n",
      "x_250.std() = 0.9738093018531799\n",
      "x_250.mean() = 0.2834331691265106\n",
      "x_300.std() = 1.0011403560638428\n",
      "x_300.mean() = 0.1843012273311615\n",
      "x_350.std() = 0.9988614320755005\n",
      "x_350.mean() = 0.1509888619184494\n",
      "x_400.std() = 0.9909386038780212\n",
      "x_400.mean() = 0.12187229096889496\n",
      "x_450.std() = 0.9773322343826294\n",
      "x_450.mean() = 0.05976749584078789\n",
      "x_500.std() = 1.0135698318481445\n",
      "x_500.mean() = 0.045845091342926025\n"
     ]
    }
   ],
   "source": [
    "model = UNetModel(resolution, input_nc, 128, input_nc, blur = blur, dropout=opt.dropout, freq_feat = opt.freq_feat)\n",
    "if not opt.ckpt == '' and os.path.exists(opt.ckpt):\n",
    "    model.load_state_dict(torch.load(opt.ckpt))\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
    "    model = DataParallel(model,device_ids=[0,1,2])\n",
    "\n",
    "model.to(device)\n",
    "print(\"input_nc\", input_nc, \"resolution\", resolution)\n",
    "\n",
    "# data_loader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "#                                           batch_size=bsize,\n",
    "#                                           shuffle=True,\n",
    "#                                           drop_last=True)\n",
    "# data_loader_test = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "#                                                batch_size=bsize,\n",
    "#                                                shuffle=False,\n",
    "#                                                drop_last=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "if opt.use_ema:\n",
    "    optimizer = EMA(optimizer, ema_decay=opt.ema_decay)\n",
    "\n",
    "# forward process visualization\n",
    "sample = train_dataset[1][0].unsqueeze(0)\n",
    "\n",
    "x_0 = sample[:4]\n",
    "x_0 = x_0.to(device)\n",
    "i = np.array([500] * x_0.shape[0])\n",
    "i = torch.from_numpy(i).to(device)\n",
    "fb.sanity(x_0, i)\n",
    "\n",
    "sample_list = []\n",
    "for i in range(0, N+1, N//10):\n",
    "    if i == 0:\n",
    "        sample_list.append(x_0[0])\n",
    "        continue\n",
    "    i = np.array([i] * x_0.shape[0])\n",
    "    i = torch.from_numpy(i).to(device)\n",
    "    x_i = fb.get_x_i(x_0, i)\n",
    "    sample_list.append(x_i[0])\n",
    "    print(f\"x_{i.item()}.std() = {x_i.std()}\")\n",
    "    print(f\"x_{i.item()}.mean() = {x_i.mean()}\")\n",
    "\n",
    "\n",
    "grid_sample = torch.cat(sample_list, dim=2)\n",
    "utils.tensor_imsave(grid_sample, \"./\" + dir, \"forward_process.jpg\")\n",
    "with open(os.path.join(dir, \"config.json\"), \"w\") as json_file:\n",
    "    json.dump(vars(opt), json_file)\n",
    "import time\n",
    "meta_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f395a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.3302, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.6000, device='cuda:0', grad_fn=<SubBackward0>) 0.7302102880077691\n",
      "100 tensor(0.5038, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0093, device='cuda:0', grad_fn=<SubBackward0>) 0.4944978975924776\n",
      "200 tensor(0.4481, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.3375, device='cuda:0', grad_fn=<SubBackward0>) 0.7856205155013396\n",
      "300 tensor(0.1486, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1006, device='cuda:0', grad_fn=<SubBackward0>) 0.2492219620010407\n",
      "400 tensor(0.4680, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.4041, device='cuda:0', grad_fn=<SubBackward0>) 0.8721190609295526\n",
      "500 tensor(0.1884, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1516, device='cuda:0', grad_fn=<SubBackward0>) 0.33998056227822926\n",
      "600 tensor(0.0478, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0337, device='cuda:0', grad_fn=<SubBackward0>) 0.08156196135102849\n",
      "700 tensor(0.0566, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0465, device='cuda:0', grad_fn=<SubBackward0>) 0.1031194243480529\n",
      "800 tensor(0.2988, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.2692, device='cuda:0', grad_fn=<SubBackward0>) 0.5679295817637047\n",
      "900 tensor(0.0418, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0317, device='cuda:0', grad_fn=<SubBackward0>) 0.07354982503521544\n",
      "1000 tensor(0.1584, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.1476, device='cuda:0', grad_fn=<SubBackward0>) 0.30604726289494505\n",
      "pred.std() = 0.9948745965957642\n",
      "pred.std() = 0.9989993572235107\n",
      "step: 1000 loss: 0.3109666705131531\n",
      "1100 tensor(0.1285, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0634, device='cuda:0', grad_fn=<SubBackward0>) 0.06509929882772364\n",
      "1200 tensor(0.0308, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0015, device='cuda:0', grad_fn=<SubBackward0>) 0.032275116644494645\n",
      "1300 tensor(0.0610, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0337, device='cuda:0', grad_fn=<SubBackward0>) 0.027224245443202937\n",
      "1400 tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0004, device='cuda:0', grad_fn=<SubBackward0>) 0.020281832430982624\n",
      "1500 tensor(0.0246, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0040, device='cuda:0', grad_fn=<SubBackward0>) 0.028623327145266036\n",
      "1600 tensor(0.0380, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0004, device='cuda:0', grad_fn=<SubBackward0>) 0.03838301743191869\n",
      "1700 tensor(0.0293, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0008, device='cuda:0', grad_fn=<SubBackward0>) 0.030093821863509954\n",
      "1800 tensor(0.0393, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0181, device='cuda:0', grad_fn=<SubBackward0>) 0.05740818866836689\n",
      "1900 tensor(0.0259, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0015, device='cuda:0', grad_fn=<SubBackward0>) 0.02737751535552989\n",
      "2000 tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0039, device='cuda:0', grad_fn=<SubBackward0>) 0.030538831271414588\n",
      "pred.std() = 0.994759202003479\n",
      "pred.std() = 1.0016885995864868\n",
      "step: 2000 loss: 0.27718737721443176\n",
      "2100 tensor(0.0510, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0140, device='cuda:0', grad_fn=<SubBackward0>) 0.037034758297837166\n",
      "2200 tensor(0.0390, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0052, device='cuda:0', grad_fn=<SubBackward0>) 0.03383633628984666\n",
      "2300 tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0048, device='cuda:0', grad_fn=<SubBackward0>) 0.03233301387714873\n",
      "2400 tensor(0.0313, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0011, device='cuda:0', grad_fn=<SubBackward0>) 0.030162396830934607\n",
      "path: experiments/blur_diff/2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 49.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2400, fid = 273.0727353966689\n",
      "2500 tensor(0.1169, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0660, device='cuda:0', grad_fn=<SubBackward0>) 0.05088943350815026\n",
      "2600 tensor(0.0319, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0057, device='cuda:0', grad_fn=<SubBackward0>) 0.026263388750915412\n",
      "2700 tensor(0.0282, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0149, device='cuda:0', grad_fn=<SubBackward0>) 0.04311267051034763\n",
      "2800 tensor(0.0244, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0072, device='cuda:0', grad_fn=<SubBackward0>) 0.0316743702120024\n",
      "2900 tensor(0.0128, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0071, device='cuda:0', grad_fn=<SubBackward0>) 0.01985418374952834\n",
      "3000 tensor(0.0529, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0241, device='cuda:0', grad_fn=<SubBackward0>) 0.02884554607317023\n",
      "pred.std() = 1.0023400783538818\n",
      "pred.std() = 0.996441125869751\n",
      "step: 3000 loss: 0.3501817584037781\n",
      "3100 tensor(0.0325, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0023, device='cuda:0', grad_fn=<SubBackward0>) 0.034842407875633453\n",
      "3200 tensor(0.0379, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0026, device='cuda:0', grad_fn=<SubBackward0>) 0.03528648598444211\n",
      "3300 tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0024, device='cuda:0', grad_fn=<SubBackward0>) 0.016791446018986782\n",
      "3400 tensor(0.0639, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0280, device='cuda:0', grad_fn=<SubBackward0>) 0.03599151013300649\n",
      "3500 tensor(0.0650, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0365, device='cuda:0', grad_fn=<SubBackward0>) 0.028475757342635907\n",
      "3600 tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0105, device='cuda:0', grad_fn=<SubBackward0>) 0.02823784141163055\n",
      "3700 tensor(0.0354, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<SubBackward0>) 0.034228494021443734\n",
      "3800 tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0131, device='cuda:0', grad_fn=<SubBackward0>) 0.041536752847614065\n",
      "3900 tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0139, device='cuda:0', grad_fn=<SubBackward0>) 0.03954009289037689\n",
      "4000 tensor(0.0270, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0074, device='cuda:0', grad_fn=<SubBackward0>) 0.03435745950060524\n",
      "path: experiments/blur_diff/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 49.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, fid = 264.3244701823735\n",
      "pred.std() = 0.9967623949050903\n",
      "pred.std() = 0.9949722290039062\n",
      "step: 4000 loss: 0.336889386177063\n",
      "4100 tensor(0.0342, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0012, device='cuda:0', grad_fn=<SubBackward0>) 0.033057343636145875\n",
      "4200 tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0242, device='cuda:0', grad_fn=<SubBackward0>) 0.019475105539200954\n",
      "4300 tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0369, device='cuda:0', grad_fn=<SubBackward0>) 0.03649837748462682\n",
      "4400 tensor(0.0365, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0070, device='cuda:0', grad_fn=<SubBackward0>) 0.04351182119715914\n",
      "4500 tensor(0.0171, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0088, device='cuda:0', grad_fn=<SubBackward0>) 0.025852617796296246\n",
      "4600 tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0096, device='cuda:0', grad_fn=<SubBackward0>) 0.04138920393091185\n",
      "4700 tensor(0.0414, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0009, device='cuda:0', grad_fn=<SubBackward0>) 0.04049406022122532\n",
      "4800 tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0057, device='cuda:0', grad_fn=<SubBackward0>) 0.02392189129126911\n",
      "4900 tensor(0.0391, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0012, device='cuda:0', grad_fn=<SubBackward0>) 0.04028649204473828\n",
      "5000 tensor(0.0436, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0173, device='cuda:0', grad_fn=<SubBackward0>) 0.02632670570518978\n",
      "pred.std() = 0.9994633793830872\n",
      "pred.std() = 1.0068910121917725\n",
      "step: 5000 loss: 0.28783440589904785\n",
      "5100 tensor(0.0470, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0037, device='cuda:0', grad_fn=<SubBackward0>) 0.04326440424714517\n",
      "5200 tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0077, device='cuda:0', grad_fn=<SubBackward0>) 0.02870865519180517\n",
      "5300 tensor(0.0150, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0084, device='cuda:0', grad_fn=<SubBackward0>) 0.02338855671505435\n",
      "5400 tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0030, device='cuda:0', grad_fn=<SubBackward0>) 0.034056971952638314\n",
      "5500 tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0092, device='cuda:0', grad_fn=<SubBackward0>) 0.027161866830385097\n",
      "5600 tensor(0.0216, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0097, device='cuda:0', grad_fn=<SubBackward0>) 0.03130999693761393\n",
      "5700 tensor(0.0396, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0191, device='cuda:0', grad_fn=<SubBackward0>) 0.058656791268654064\n",
      "5800 tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0060, device='cuda:0', grad_fn=<SubBackward0>) 0.023539949596040614\n",
      "5900 tensor(0.0268, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0025, device='cuda:0', grad_fn=<SubBackward0>) 0.029324428001511763\n",
      "6000 tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0241, device='cuda:0', grad_fn=<SubBackward0>) 0.06647468820531277\n",
      "path: experiments/blur_diff/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, fid = 229.99878325874857\n",
      "pred.std() = 1.0082104206085205\n",
      "pred.std() = 1.0039129257202148\n",
      "step: 6000 loss: 0.30954107642173767\n",
      "6100 tensor(0.0592, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0052, device='cuda:0', grad_fn=<SubBackward0>) 0.06442813928751577\n",
      "6200 tensor(0.0262, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0126, device='cuda:0', grad_fn=<SubBackward0>) 0.038785531684942774\n",
      "6300 tensor(0.0247, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0101, device='cuda:0', grad_fn=<SubBackward0>) 0.03482856174872956\n",
      "6400 tensor(0.0208, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0134, device='cuda:0', grad_fn=<SubBackward0>) 0.03420890456691399\n",
      "6500 tensor(0.0481, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0103, device='cuda:0', grad_fn=<SubBackward0>) 0.03777685412345786\n",
      "6600 tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0124, device='cuda:0', grad_fn=<SubBackward0>) 0.03429055834933888\n",
      "6700 tensor(0.0189, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0116, device='cuda:0', grad_fn=<SubBackward0>) 0.0305470072141992\n",
      "6800 tensor(0.0285, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0122, device='cuda:0', grad_fn=<SubBackward0>) 0.04065826635407873\n",
      "6900 tensor(0.0233, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0054, device='cuda:0', grad_fn=<SubBackward0>) 0.02873845466869408\n",
      "7000 tensor(0.0678, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0222, device='cuda:0', grad_fn=<SubBackward0>) 0.09007434876658797\n",
      "pred.std() = 1.0067882537841797\n",
      "pred.std() = 1.0061297416687012\n",
      "step: 7000 loss: 0.32368117570877075\n",
      "7100 tensor(0.0193, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0020, device='cuda:0', grad_fn=<SubBackward0>) 0.021352292544314377\n",
      "7200 tensor(0.0160, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0069, device='cuda:0', grad_fn=<SubBackward0>) 0.022858434333702004\n",
      "7300 tensor(0.0176, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0066, device='cuda:0', grad_fn=<SubBackward0>) 0.02421052562961285\n",
      "7400 tensor(0.0318, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0055, device='cuda:0', grad_fn=<SubBackward0>) 0.037269860652687864\n",
      "7500 tensor(0.0648, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0300, device='cuda:0', grad_fn=<SubBackward0>) 0.09485661266890878\n",
      "7600 tensor(0.0275, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0067, device='cuda:0', grad_fn=<SubBackward0>) 0.020840382580306224\n",
      "7700 tensor(0.0423, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0218, device='cuda:0', grad_fn=<SubBackward0>) 0.020518098731623063\n",
      "7800 tensor(0.0253, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0042, device='cuda:0', grad_fn=<SubBackward0>) 0.029512177911160647\n",
      "7900 tensor(0.0225, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0136, device='cuda:0', grad_fn=<SubBackward0>) 0.03608940612887352\n",
      "8000 tensor(0.0172, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0078, device='cuda:0', grad_fn=<SubBackward0>) 0.024993173416817217\n",
      "path: experiments/blur_diff/8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 51.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, fid = 224.55480968818586\n",
      "pred.std() = 0.9976666569709778\n",
      "pred.std() = 0.9968157410621643\n",
      "step: 8000 loss: 0.3710719048976898\n",
      "8100 tensor(0.0132, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0028, device='cuda:0', grad_fn=<SubBackward0>) 0.01600756486471465\n",
      "8200 tensor(0.0127, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0063, device='cuda:0', grad_fn=<SubBackward0>) 0.019000161943111857\n",
      "8300 tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0037, device='cuda:0', grad_fn=<SubBackward0>) 0.018375803907713796\n",
      "8400 tensor(0.0241, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0032, device='cuda:0', grad_fn=<SubBackward0>) 0.027298409468686672\n",
      "8500 tensor(0.0181, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0104, device='cuda:0', grad_fn=<SubBackward0>) 0.028488340998373943\n",
      "8600 tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0573, device='cuda:0', grad_fn=<SubBackward0>) 0.021629861197617655\n",
      "8700 tensor(0.0269, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0121, device='cuda:0', grad_fn=<SubBackward0>) 0.038969491102600194\n",
      "8800 tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0060, device='cuda:0', grad_fn=<SubBackward0>) 0.08282077611202741\n",
      "8900 tensor(0.0204, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0045, device='cuda:0', grad_fn=<SubBackward0>) 0.024833393147426785\n",
      "9000 tensor(0.0601, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0244, device='cuda:0', grad_fn=<SubBackward0>) 0.03563726259598771\n",
      "pred.std() = 0.9933664202690125\n",
      "pred.std() = 1.0059552192687988\n",
      "step: 9000 loss: 0.2849498987197876\n",
      "9100 tensor(0.0207, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0009, device='cuda:0', grad_fn=<SubBackward0>) 0.021564773595818543\n",
      "9200 tensor(0.0954, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0083, device='cuda:0', grad_fn=<SubBackward0>) 0.10371855563662778\n",
      "9300 tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0411, device='cuda:0', grad_fn=<SubBackward0>) 0.03323750100106931\n",
      "9400 tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>) tensor(-9.7621e-06, device='cuda:0', grad_fn=<SubBackward0>) 0.027185027921473145\n",
      "9500 tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0084, device='cuda:0', grad_fn=<SubBackward0>) 0.02169391032156881\n",
      "9600 tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0132, device='cuda:0', grad_fn=<SubBackward0>) 0.039838866718771244\n",
      "9700 tensor(0.0645, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0376, device='cuda:0', grad_fn=<SubBackward0>) 0.10211046994226183\n",
      "9800 tensor(0.0352, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0215, device='cuda:0', grad_fn=<SubBackward0>) 0.056699417957744094\n",
      "9900 tensor(0.0215, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0062, device='cuda:0', grad_fn=<SubBackward0>) 0.027714430210700327\n",
      "10000 tensor(0.0126, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0070, device='cuda:0', grad_fn=<SubBackward0>) 0.019600079913076232\n",
      "path: experiments/blur_diff/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 49.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, fid = 233.82995905866238\n",
      "pred.std() = 0.9999263286590576\n",
      "pred.std() = 1.0010802745819092\n",
      "step: 10000 loss: 0.2695009708404541\n",
      "10100 tensor(0.0448, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0057, device='cuda:0', grad_fn=<SubBackward0>) 0.05055498313183825\n",
      "10200 tensor(0.0289, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0019, device='cuda:0', grad_fn=<SubBackward0>) 0.03080775985831232\n",
      "10300 tensor(0.0131, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0044, device='cuda:0', grad_fn=<SubBackward0>) 0.017448374603820035\n",
      "10400 tensor(0.0185, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0077, device='cuda:0', grad_fn=<SubBackward0>) 0.026215929714746105\n",
      "10500 tensor(0.0162, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0066, device='cuda:0', grad_fn=<SubBackward0>) 0.022792802852890835\n",
      "10600 tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0065, device='cuda:0', grad_fn=<SubBackward0>) 0.024410212601361492\n",
      "10700 tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0139, device='cuda:0', grad_fn=<SubBackward0>) 0.039348800886812176\n",
      "10800 tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0463, device='cuda:0', grad_fn=<SubBackward0>) 0.02523447404508952\n",
      "10900 tensor(0.0154, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0026, device='cuda:0', grad_fn=<SubBackward0>) 0.018059821234163418\n",
      "11000 tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0076, device='cuda:0', grad_fn=<SubBackward0>) 0.02287878645455131\n",
      "pred.std() = 0.9909385442733765\n",
      "pred.std() = 1.007488489151001\n",
      "step: 11000 loss: 0.28757762908935547\n",
      "11100 tensor(0.0410, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0059, device='cuda:0', grad_fn=<SubBackward0>) 0.0351472372113348\n",
      "11200 tensor(0.0385, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0077, device='cuda:0', grad_fn=<SubBackward0>) 0.04627552339348377\n",
      "11300 tensor(0.0152, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0067, device='cuda:0', grad_fn=<SubBackward0>) 0.021949735923168963\n",
      "11400 tensor(0.0133, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0039, device='cuda:0', grad_fn=<SubBackward0>) 0.01718675622829007\n",
      "11500 tensor(0.0239, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0130, device='cuda:0', grad_fn=<SubBackward0>) 0.03690077246480975\n",
      "11600 tensor(0.0178, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0093, device='cuda:0', grad_fn=<SubBackward0>) 0.027047517241286876\n",
      "11700 tensor(0.0604, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0277, device='cuda:0', grad_fn=<SubBackward0>) 0.032705447162086496\n",
      "11800 tensor(0.0220, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0152, device='cuda:0', grad_fn=<SubBackward0>) 0.037212424230646884\n",
      "11900 tensor(0.0210, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0062, device='cuda:0', grad_fn=<SubBackward0>) 0.02725930720081061\n",
      "12000 tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0091, device='cuda:0', grad_fn=<SubBackward0>) 0.025502580431126177\n",
      "path: experiments/blur_diff/12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 48.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, fid = 247.27835399465997\n",
      "pred.std() = 1.0032517910003662\n",
      "pred.std() = 1.0130748748779297\n",
      "step: 12000 loss: 0.26516446471214294\n",
      "12100 tensor(0.0266, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0051, device='cuda:0', grad_fn=<SubBackward0>) 0.021491657314463695\n",
      "12200 tensor(0.0281, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0010, device='cuda:0', grad_fn=<SubBackward0>) 0.027027214011467873\n",
      "12300 tensor(0.0151, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0069, device='cuda:0', grad_fn=<SubBackward0>) 0.022082012649977927\n",
      "12400 tensor(0.0216, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0124, device='cuda:0', grad_fn=<SubBackward0>) 0.034065395749385996\n",
      "12500 tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0304, device='cuda:0', grad_fn=<SubBackward0>) 0.10619041101278824\n",
      "12600 tensor(0.0374, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0166, device='cuda:0', grad_fn=<SubBackward0>) 0.05399135966719743\n",
      "12700 tensor(0.0142, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0098, device='cuda:0', grad_fn=<SubBackward0>) 0.02400262021930347\n",
      "12800 tensor(0.0175, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0098, device='cuda:0', grad_fn=<SubBackward0>) 0.02729315535488147\n",
      "12900 tensor(0.0366, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0074, device='cuda:0', grad_fn=<SubBackward0>) 0.02918714689144176\n",
      "13000 tensor(0.0272, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0057, device='cuda:0', grad_fn=<SubBackward0>) 0.03291999270327729\n",
      "pred.std() = 0.9941185712814331\n",
      "pred.std() = 0.9993950724601746\n",
      "step: 13000 loss: 0.23998236656188965\n",
      "13100 tensor(0.0195, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0005, device='cuda:0', grad_fn=<SubBackward0>) 0.020014720339026685\n",
      "13200 tensor(0.0100, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0071, device='cuda:0', grad_fn=<SubBackward0>) 0.017034866777790914\n",
      "13300 tensor(0.0377, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0039, device='cuda:0', grad_fn=<SubBackward0>) 0.04157874345468764\n",
      "13400 tensor(0.0205, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0023, device='cuda:0', grad_fn=<SubBackward0>) 0.02282733560411557\n",
      "13500 tensor(0.0255, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0147, device='cuda:0', grad_fn=<SubBackward0>) 0.04019099210786309\n",
      "13600 tensor(0.0434, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0155, device='cuda:0', grad_fn=<SubBackward0>) 0.02793103839580813\n",
      "13700 tensor(0.0179, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0080, device='cuda:0', grad_fn=<SubBackward0>) 0.02592072797390075\n",
      "13800 tensor(0.0294, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0176, device='cuda:0', grad_fn=<SubBackward0>) 0.04701872915649337\n",
      "13900 tensor(0.0141, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0062, device='cuda:0', grad_fn=<SubBackward0>) 0.0202928989729999\n",
      "14000 tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0070, device='cuda:0', grad_fn=<SubBackward0>) 0.02175196229557633\n",
      "path: experiments/blur_diff/14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 50.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, fid = 215.9210782593476\n",
      "pred.std() = 1.0019195079803467\n",
      "pred.std() = 0.9920446276664734\n",
      "step: 14000 loss: 0.3014170527458191\n",
      "14100 tensor(0.0140, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0077, device='cuda:0', grad_fn=<SubBackward0>) 0.021671475170353647\n",
      "14200 tensor(0.0177, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0036, device='cuda:0', grad_fn=<SubBackward0>) 0.021213090419841537\n",
      "14300 tensor(0.0191, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0089, device='cuda:0', grad_fn=<SubBackward0>) 0.02805821828983966\n",
      "14400 tensor(0.0521, device='cuda:0', grad_fn=<AddBackward0>) tensor(0.0105, device='cuda:0', grad_fn=<SubBackward0>) 0.04158707966083814\n",
      "14500 tensor(0.0144, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0063, device='cuda:0', grad_fn=<SubBackward0>) 0.020659562716549874\n",
      "14600 tensor(0.0153, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0086, device='cuda:0', grad_fn=<SubBackward0>) 0.023925027799188554\n",
      "14700 tensor(0.0302, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0170, device='cuda:0', grad_fn=<SubBackward0>) 0.04711148780370854\n",
      "14800 tensor(0.0201, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0070, device='cuda:0', grad_fn=<SubBackward0>) 0.027044790875243984\n",
      "14900 tensor(0.0147, device='cuda:0', grad_fn=<AddBackward0>) tensor(-0.0106, device='cuda:0', grad_fn=<SubBackward0>) 0.025337022767868103\n"
     ]
    }
   ],
   "source": [
    "for step in range(opt.max_iter):\n",
    "    if not opt.inference:\n",
    "        elips = time.time()\n",
    "        try:\n",
    "            x_0, _ = train_iter.next()\n",
    "        except:\n",
    "            train_iter = iter(train_loader)\n",
    "            image, _ = next(train_iter)\n",
    "        \"\"\"\n",
    "        training\n",
    "        \"\"\"\n",
    "        assert x_0.shape[-1] == resolution, f\"{x_0.shape}\"\n",
    "        i = np.random.uniform(1 / N, 1, size = (x_0.shape[0])) * N\n",
    "        i = torch.from_numpy(i).to(device).type(torch.long)\n",
    "#         i = i.to(device).type(torch.long)\n",
    "\n",
    "        x_0 = x_0.to(device)\n",
    "        x_i, eps = fb.get_x_i(x_0, i, return_eps = True)\n",
    "        \n",
    "        if opt.loss_type == \"sm_simple\":\n",
    "            loss = fb.get_loss_i_simple(model, x_0, x_i, i)\n",
    "        elif opt.loss_type == \"eps_simple\":\n",
    "            loss = fb.get_loss_i_eps_simple(model, x_i, i, eps)\n",
    "        elif opt.loss_type == \"sm_exact\":\n",
    "            loss = fb.get_loss_i_exact(model, x_0, x_i, i)\n",
    "        elif opt.loss_type == \"std_matching\":\n",
    "            loss = fb.get_loss_i_std_matching(model, x_i, i, eps)\n",
    "        ## Contrastive Loss\n",
    "        s = model(x_i, i)\n",
    "        s = fb.U_I_minus_B_Ut(s, i)\n",
    "        hf = x_i - fb.W(x_i, i)\n",
    "        # Anderson theorem\n",
    "        pred1 = x_i + hf # unsharpening mask filtering\n",
    "        pred2 = pred1 + s \n",
    "        sim_pos = 1/np.linalg.norm(pred2.cpu().detach().numpy()-x_0.cpu().detach().numpy())\n",
    "        sim_neg = 1/np.linalg.norm(pred2.cpu().detach().numpy()-x_i.cpu().detach().numpy())\n",
    "        contrastive_loss = (sim_pos - sim_neg)**2\n",
    "        writer.add_scalar('loss_train', loss, step)\n",
    "\n",
    "        writer.add_scalar('contrastive_loss', contrastive_loss, step)\n",
    "        loss += 0.5*contrastive_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "            print(step, loss,loss-contrastive_loss,contrastive_loss)\n",
    "        # print(f\"time: {time.time() - elips}\")\n",
    "    # Calcuate FID\n",
    "    if step > 2401:\n",
    "        fid_iter = opt.fid_iter\n",
    "    else:\n",
    "        fid_iter = 2400\n",
    "    if (step % fid_iter == 0 and step > 0):\n",
    "        id = 0\n",
    "        if not os.path.exists(os.path.join(\"./\",dir, f\"{step}\")):\n",
    "            os.mkdir(os.path.join(\"./\",dir, f\"{step}\"))\n",
    "        with torch.no_grad():\n",
    "            if opt.use_ema:\n",
    "                optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.eval()\n",
    "            for _ in range(opt.fid_num_samples // opt.fid_bsize):\n",
    "                i = np.array([opt.N - 1] * opt.fid_bsize)\n",
    "                i = torch.from_numpy(i).to(device)\n",
    "                pred = fb.get_x_N([opt.fid_bsize, input_nc, resolution, resolution], i)\n",
    "                for i in reversed(range(1, opt.N)):\n",
    "                    i = np.array([i] * opt.fid_bsize)\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.loss_type == \"sm_simple\":\n",
    "                        s = model(pred, i)\n",
    "                    elif opt.loss_type == \"eps_simple\":\n",
    "                        eps = model(pred, i)\n",
    "                        s = fb.get_score_from_eps(eps, i)\n",
    "                    elif opt.loss_type == \"sm_exact\":\n",
    "                        s = model(pred, i)\n",
    "                    elif opt.loss_type == \"std_matching\":\n",
    "                        std = model(pred, i)\n",
    "                        s = fb.get_score_from_std(std, i)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    rms = lambda x: torch.sqrt(torch.mean(x ** 2))\n",
    "                    # print(f\"rms(s) * fb._beta_i(i) = {rms(s) * fb._beta_i(i)[0]}\")\n",
    "                    hf = pred - fb.W(pred, i)\n",
    "                    # Anderson theorem\n",
    "                    pred1 = pred + hf # unsharpening mask filtering\n",
    "                    pred2 = pred1 + s  # # denoising\n",
    "                    if i[0] > 2:\n",
    "                        pred = pred2 + fb.U_I_minus_B_sqrt_Ut(torch.randn_like(pred), i) # inject noise\n",
    "                    else:\n",
    "                        pred = pred2\n",
    "                    # print(f\"i = {i[0]}, rmse = {torch.sqrt(torch.mean(pred**2))}, mean = {torch.mean(pred)} std = {torch.std(pred)}\" )\n",
    "                for sample in pred:\n",
    "                    save_image(sample, os.path.join(dir, f\"{step}\", f\"{id:05d}.png\"))\n",
    "                    id += 1\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.train()\n",
    "        fid = fid_eval(os.path.join(dir, f\"{step}\"))\n",
    "        writer.add_scalar('fid', fid, step)\n",
    "        print(f\"step {step}, fid = {fid}\")\n",
    "    if (step % opt.eval_iter == 0 and step > 0) or opt.inference:\n",
    "        \"\"\"\n",
    "        sampling (eval)\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            if opt.use_ema:\n",
    "                optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.eval()\n",
    "\n",
    "            if opt.ode:\n",
    "                raise NotImplementedError\n",
    "                def to_flattened_numpy(x):\n",
    "                    \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n",
    "                    return x.detach().cpu().numpy().reshape((-1,))\n",
    "                def from_flattened_numpy(x, shape):\n",
    "                    \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n",
    "                    return torch.from_numpy(x.reshape(shape))\n",
    "                def ode_func(i, y):\n",
    "                    i = int(i*N)\n",
    "                    print(f\"i = {i}\")\n",
    "                    y = from_flattened_numpy(y, [bsize, input_nc, resolution, resolution]).to(device).type(torch.float32)\n",
    "                    i = np.array([N - 1] * bsize)\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.loss_type == \"sm_simple\":\n",
    "                            s = model(y, i)\n",
    "                    elif opt.loss_type == \"eps_simple\":\n",
    "                        eps = model(y, i)\n",
    "                        s = fb.get_score_from_eps(eps, i)\n",
    "                    elif opt.loss_type == \"sm_exact\":\n",
    "                        s = model(y, i)\n",
    "                    elif opt.loss_type == \"std_matching\":\n",
    "                        std = model(y, i)\n",
    "                        s = fb.get_score_from_std(std, i)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    hf = y - fb.W(y, i)\n",
    "                    dt = - 1.0 / N\n",
    "                    drift = (s/2 + hf) / dt\n",
    "                    drift = to_flattened_numpy(drift)\n",
    "                    return drift\n",
    "                x_N = fb.get_x_N([bsize, input_nc, resolution, resolution], N)\n",
    "                solution = solve_ivp(ode_func, (1, 1e-3), to_flattened_numpy(x_N),\n",
    "                                     rtol=1e-3, atol=1e-3, method=\"RK45\")\n",
    "                nfe = solution.nfev\n",
    "                solution = torch.tensor(solution.y[:, -1]).reshape(x_N.shape).to(device).type(torch.float32)\n",
    "                \n",
    "                save_image(solution, \"./solution.jpg\")\n",
    "                print(f\"nfe = {nfe}\")\n",
    "                raise NotImplementedError\n",
    "            for x_0, _ in test_loader:\n",
    "                x_0 = x_0.to(device)\n",
    "                # for v in range(0, 250, 20):\n",
    "                #     x_0[:, :, v, :] = 0\n",
    "                if opt.fromprior:\n",
    "                    i = np.array([N - 1] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    pred = fb.get_x_N(x_0.shape, i)\n",
    "                    print(f\"pred.std() = {pred.std()}\")\n",
    "                else:\n",
    "                    i = np.array([N-1] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    pred = fb.get_x_i(x_0, i)\n",
    "                preds = [pred]\n",
    "\n",
    "                for i in reversed(range(1, N)):\n",
    "                    i = np.array([i] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.gtscore:\n",
    "                        s = fb.get_score_gt(pred, x_0, i)\n",
    "                    else:\n",
    "                        if opt.loss_type == \"sm_simple\":\n",
    "                            s = model(pred, i)\n",
    "                        elif opt.loss_type == \"eps_simple\":\n",
    "                            eps = model(pred, i)\n",
    "                            s = fb.get_score_from_eps(eps, i)\n",
    "                        elif opt.loss_type == \"sm_exact\":\n",
    "                            s = model(pred, i)\n",
    "                        elif opt.loss_type == \"std_matching\":\n",
    "                            std = model(pred, i)\n",
    "                            s = fb.get_score_from_std(std, i)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    rms = lambda x: torch.sqrt(torch.mean(x ** 2))\n",
    "                    # print(f\"rms(s) * fb._beta_i(i) = {rms(s) * fb._beta_i(i)[0]}\")\n",
    "                    hf = pred - fb.W(pred, i)\n",
    "                    # Anderson theorem\n",
    "                    pred1 = pred + hf # unsharpening mask filtering\n",
    "                    pred2 = pred1 + s  # # denoising\n",
    "                    if i[0] > 2:\n",
    "                        pred = pred2 + fb.U_I_minus_B_sqrt_Ut(torch.randn_like(pred), i) # inject noise\n",
    "                    else:\n",
    "                        pred = pred2\n",
    "                    # print(f\"i = {i[0]}, rmse = {torch.sqrt(torch.mean(pred**2))}, mean = {torch.mean(pred)} std = {torch.std(pred)}\")\n",
    "                    # assert rms(pred) < 100\n",
    "                    if (i[0]) % (N // 10) == 0:\n",
    "                        img = pred[0]\n",
    "                        preds.append(pred)\n",
    "\n",
    "                preds.append(pred)\n",
    "                assert x_0.shape == pred.shape\n",
    "                # visualize\n",
    "                grid = torch.cat(preds, dim=3) # grid_sample.shape: (bsize, channel, H, W * 12)\n",
    "                # (batch_size, channel, H, W * 12) -> (channel, H * bsize, W * 12)\n",
    "                grid = grid.permute(1, 0, 2, 3).contiguous().view(grid.shape[1], -1, grid.shape[3])\n",
    "                # (bsize, channel, H, W) -> (channel, H, W * bsize)\n",
    "                gt = x_0.permute(1, 2, 0, 3).contiguous().view(x_0.shape[1], -1, x_0.shape[3] * x_0.shape[0])\n",
    "                if cnt <= 2:\n",
    "                    utils.tensor_imsave(gt, \"./\" + dir, f\"{step}_{cnt}_GT.jpg\")\n",
    "                    utils.tensor_imsave(grid, \"./\" + dir, f\"{step}_{cnt}_pred.jpg\")\n",
    "              \n",
    "                cnt += 1\n",
    "                loss += TF.l1_loss(x_0, pred) / 2\n",
    "\n",
    "                if cnt == 2:\n",
    "                    break\n",
    "        print(f\"step: {step} loss: {loss}\")\n",
    "        writer.add_scalar('loss_val', loss, meta_iter)\n",
    "        f = open('./' + str(dir) + '/log.txt', 'a')\n",
    "\n",
    "        f.write(f\"Step: {step} loss: {loss}\" + '\\n')\n",
    "\n",
    "        f.close()\n",
    "        model.train()\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "    if step % opt.save_every == 1:\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            torch.save(model.module.state_dict(), os.path.join(dir, f\"model_{step}.ckpt\"))\n",
    "        else:\n",
    "            torch.save(model.state_dict(), os.path.join(dir, f\"model_{step}.ckpt\"))\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a07a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "100 tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "200 tensor(0.0173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "300 tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "400 tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "500 tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "600 tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "700 tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "800 tensor(0.0018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "900 tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1000 tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 1.006337285041809\n",
      "pred.std() = 0.9932403564453125\n",
      "step: 1000 loss: 0.26761966943740845\n",
      "1100 tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1200 tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1300 tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1400 tensor(0.0274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1500 tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1600 tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1700 tensor(0.0304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1800 tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "1900 tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2000 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 0.9940692782402039\n",
      "pred.std() = 0.999826192855835\n",
      "step: 2000 loss: 0.2694593667984009\n",
      "2100 tensor(0.0288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2200 tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2300 tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2400 tensor(0.0132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 50.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2400, fid = 263.15244523137915\n",
      "2500 tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2600 tensor(0.0217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2700 tensor(0.0059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2800 tensor(0.0025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "2900 tensor(0.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3000 tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 0.9993970394134521\n",
      "pred.std() = 1.0008867979049683\n",
      "step: 3000 loss: 0.2980382442474365\n",
      "3100 tensor(0.0126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3200 tensor(0.0075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3300 tensor(0.0105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3400 tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3500 tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3600 tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3700 tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3800 tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "3900 tensor(0.0224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4000 tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 49.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, fid = 240.19906476128102\n",
      "pred.std() = 1.001698613166809\n",
      "pred.std() = 1.0025451183319092\n",
      "step: 4000 loss: 0.2672337591648102\n",
      "4100 tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4200 tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4300 tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4400 tensor(0.0064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4500 tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4600 tensor(0.0042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4700 tensor(0.0070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4800 tensor(0.0192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "4900 tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5000 tensor(0.0098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 0.9998305439949036\n",
      "pred.std() = 1.0077474117279053\n",
      "step: 5000 loss: 0.24289584159851074\n",
      "5100 tensor(0.0142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5200 tensor(0.0068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5300 tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5400 tensor(0.0137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5500 tensor(0.0069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5600 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5700 tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5800 tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "5900 tensor(0.0036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6000 tensor(0.0078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 49.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, fid = 251.79785457737836\n",
      "pred.std() = 1.0009417533874512\n",
      "pred.std() = 0.9997055530548096\n",
      "step: 6000 loss: 0.23946842551231384\n",
      "6100 tensor(0.0061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6200 tensor(0.0239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6300 tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6400 tensor(0.0246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6500 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6600 tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6700 tensor(0.0109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6800 tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "6900 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7000 tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 0.9974673390388489\n",
      "pred.std() = 1.0030509233474731\n",
      "step: 7000 loss: 0.2558271586894989\n",
      "7100 tensor(0.0122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7200 tensor(0.0112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7300 tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7400 tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7500 tensor(0.0067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7600 tensor(0.0019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7700 tensor(0.0052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7800 tensor(0.0115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "7900 tensor(0.0091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8000 tensor(0.0009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 52.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, fid = 238.24954213123374\n",
      "pred.std() = 1.0023409128189087\n",
      "pred.std() = 1.0020124912261963\n",
      "step: 8000 loss: 0.25425976514816284\n",
      "8100 tensor(0.0063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8200 tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8300 tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8400 tensor(0.0087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8500 tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8600 tensor(0.0037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8700 tensor(0.0008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8800 tensor(0.0051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "8900 tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9000 tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 1.0003019571304321\n",
      "pred.std() = 1.0041351318359375\n",
      "step: 9000 loss: 0.2618172764778137\n",
      "9100 tensor(0.0190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9200 tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9300 tensor(0.0073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9400 tensor(0.0034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9500 tensor(0.0043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9600 tensor(0.0050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9700 tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9800 tensor(0.0021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "9900 tensor(0.0131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10000 tensor(0.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 50.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, fid = 242.08618148383653\n",
      "pred.std() = 0.9932202696800232\n",
      "pred.std() = 1.0001039505004883\n",
      "step: 10000 loss: 0.32482099533081055\n",
      "10100 tensor(0.0086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10200 tensor(0.0084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10300 tensor(0.0311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10400 tensor(0.0083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10500 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10600 tensor(0.0035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10700 tensor(0.0017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10800 tensor(0.0024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "10900 tensor(0.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11000 tensor(0.0414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 1.0065643787384033\n",
      "pred.std() = 0.9918767809867859\n",
      "step: 11000 loss: 0.3034140467643738\n",
      "11100 tensor(0.0251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11200 tensor(0.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11300 tensor(0.0096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11400 tensor(0.0200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11500 tensor(0.0044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11600 tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11700 tensor(0.0054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11800 tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "11900 tensor(0.0022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12000 tensor(0.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 52.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, fid = 254.29508877499453\n",
      "pred.std() = 0.9956225156784058\n",
      "pred.std() = 1.0104796886444092\n",
      "step: 12000 loss: 0.22754083573818207\n",
      "12100 tensor(0.0300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12200 tensor(0.0220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12300 tensor(0.0049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12400 tensor(0.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12500 tensor(0.0041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12600 tensor(0.0032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12700 tensor(0.0007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12800 tensor(0.0020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "12900 tensor(0.0030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13000 tensor(0.0016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "pred.std() = 0.9944551587104797\n",
      "pred.std() = 1.0035085678100586\n",
      "step: 13000 loss: 0.26888251304626465\n",
      "13100 tensor(0.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13200 tensor(0.0275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13300 tensor(0.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13400 tensor(0.0014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13500 tensor(0.0028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13600 tensor(0.0247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13700 tensor(0.0562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13800 tensor(0.0033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "13900 tensor(0.0038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14000 tensor(0.0047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "path: experiments/blur_diff/14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, fid = 275.4149450715353\n",
      "pred.std() = 0.9996069669723511\n",
      "pred.std() = 0.9931371808052063\n",
      "step: 14000 loss: 0.3260786831378937\n",
      "14100 tensor(0.0213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14200 tensor(0.0053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14300 tensor(0.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14400 tensor(0.0056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14500 tensor(0.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14600 tensor(0.0046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14700 tensor(0.0040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14800 tensor(0.0136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "14900 tensor(0.0416, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for step in range(opt.max_iter):\n",
    "    if not opt.inference:\n",
    "        elips = time.time()\n",
    "        try:\n",
    "            x_0, _ = train_iter.next()\n",
    "        except:\n",
    "            train_iter = iter(train_loader)\n",
    "            image, _ = next(train_iter)\n",
    "        \"\"\"\n",
    "        training\n",
    "        \"\"\"\n",
    "        assert x_0.shape[-1] == resolution, f\"{x_0.shape}\"\n",
    "        i = np.random.uniform(1 / N, 1, size = (x_0.shape[0])) * N\n",
    "        i = torch.from_numpy(i).to(device).type(torch.long)\n",
    "#         i = i.to(device).type(torch.long)\n",
    "\n",
    "        x_0 = x_0.to(device)\n",
    "        x_i, eps = fb.get_x_i(x_0, i, return_eps = True)\n",
    "        \n",
    "        if opt.loss_type == \"sm_simple\":\n",
    "            loss = fb.get_loss_i_simple(model, x_0, x_i, i)\n",
    "        elif opt.loss_type == \"eps_simple\":\n",
    "            loss = fb.get_loss_i_eps_simple(model, x_i, i, eps)\n",
    "        elif opt.loss_type == \"sm_exact\":\n",
    "            loss = fb.get_loss_i_exact(model, x_0, x_i, i)\n",
    "        elif opt.loss_type == \"std_matching\":\n",
    "            loss = fb.get_loss_i_std_matching(model, x_i, i, eps)\n",
    "        ## Contrastive Loss\n",
    "#         s = model(x_i, i)\n",
    "#         s = fb.U_I_minus_B_Ut(s, i)\n",
    "#         hf = x_i - fb.W(x_i, i)\n",
    "#         # Anderson theorem\n",
    "#         pred1 = x_i + hf # unsharpening mask filtering\n",
    "#         pred2 = pred1 + s \n",
    "#         sim_pos = 1/np.linalg.norm(pred2.cpu().detach().numpy()-x_0.cpu().detach().numpy())\n",
    "#         sim_neg = 1/np.linalg.norm(pred2.cpu().detach().numpy()-x_i.cpu().detach().numpy())\n",
    "#         contrastive_loss = (sim_pos - sim_neg)**2\n",
    "        writer.add_scalar('loss_train', loss, step)\n",
    "\n",
    "#         writer.add_scalar('contrastive_loss', contrastive_loss, step)\n",
    "#         loss += 0.5*contrastive_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step % 100 == 0:\n",
    "#             print(step, loss,loss-contrastive_loss,contrastive_loss)\n",
    "            print(step,loss)\n",
    "        # print(f\"time: {time.time() - elips}\")\n",
    "    # Calcuate FID\n",
    "    if step > 2401:\n",
    "        fid_iter = opt.fid_iter\n",
    "    else:\n",
    "        fid_iter = 2400\n",
    "    if (step % fid_iter == 0 and step > 0):\n",
    "        id = 0\n",
    "        if not os.path.exists(os.path.join(\"./\",dir, f\"{step}\")):\n",
    "            os.mkdir(os.path.join(\"./\",dir, f\"{step}\"))\n",
    "        with torch.no_grad():\n",
    "            if opt.use_ema:\n",
    "                optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.eval()\n",
    "            for _ in range(opt.fid_num_samples // opt.fid_bsize):\n",
    "                i = np.array([opt.N - 1] * opt.fid_bsize)\n",
    "                i = torch.from_numpy(i).to(device)\n",
    "                pred = fb.get_x_N([opt.fid_bsize, input_nc, resolution, resolution], i)\n",
    "                for i in reversed(range(1, opt.N)):\n",
    "                    i = np.array([i] * opt.fid_bsize)\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.loss_type == \"sm_simple\":\n",
    "                        s = model(pred, i)\n",
    "                    elif opt.loss_type == \"eps_simple\":\n",
    "                        eps = model(pred, i)\n",
    "                        s = fb.get_score_from_eps(eps, i)\n",
    "                    elif opt.loss_type == \"sm_exact\":\n",
    "                        s = model(pred, i)\n",
    "                    elif opt.loss_type == \"std_matching\":\n",
    "                        std = model(pred, i)\n",
    "                        s = fb.get_score_from_std(std, i)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    rms = lambda x: torch.sqrt(torch.mean(x ** 2))\n",
    "                    # print(f\"rms(s) * fb._beta_i(i) = {rms(s) * fb._beta_i(i)[0]}\")\n",
    "                    hf = pred - fb.W(pred, i)\n",
    "                    # Anderson theorem\n",
    "                    pred1 = pred + hf # unsharpening mask filtering\n",
    "                    pred2 = pred1 + s  # # denoising\n",
    "                    if i[0] > 2:\n",
    "                        pred = pred2 + fb.U_I_minus_B_sqrt_Ut(torch.randn_like(pred), i) # inject noise\n",
    "                    else:\n",
    "                        pred = pred2\n",
    "                    # print(f\"i = {i[0]}, rmse = {torch.sqrt(torch.mean(pred**2))}, mean = {torch.mean(pred)} std = {torch.std(pred)}\" )\n",
    "                for sample in pred:\n",
    "                    save_image(sample, os.path.join(dir, f\"{step}\", f\"{id:05d}.png\"))\n",
    "                    id += 1\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.train()\n",
    "        fid = fid_eval(os.path.join(dir, f\"{step}\"))\n",
    "        writer.add_scalar('fid', fid, step)\n",
    "        print(f\"step {step}, fid = {fid}\")\n",
    "    if (step % opt.eval_iter == 0 and step > 0) or opt.inference:\n",
    "        \"\"\"\n",
    "        sampling (eval)\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        loss = 0\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            if opt.use_ema:\n",
    "                optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "            model = model.eval()\n",
    "\n",
    "            if opt.ode:\n",
    "                raise NotImplementedError\n",
    "                def to_flattened_numpy(x):\n",
    "                    \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n",
    "                    return x.detach().cpu().numpy().reshape((-1,))\n",
    "                def from_flattened_numpy(x, shape):\n",
    "                    \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n",
    "                    return torch.from_numpy(x.reshape(shape))\n",
    "                def ode_func(i, y):\n",
    "                    i = int(i*N)\n",
    "                    print(f\"i = {i}\")\n",
    "                    y = from_flattened_numpy(y, [bsize, input_nc, resolution, resolution]).to(device).type(torch.float32)\n",
    "                    i = np.array([N - 1] * bsize)\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.loss_type == \"sm_simple\":\n",
    "                            s = model(y, i)\n",
    "                    elif opt.loss_type == \"eps_simple\":\n",
    "                        eps = model(y, i)\n",
    "                        s = fb.get_score_from_eps(eps, i)\n",
    "                    elif opt.loss_type == \"sm_exact\":\n",
    "                        s = model(y, i)\n",
    "                    elif opt.loss_type == \"std_matching\":\n",
    "                        std = model(y, i)\n",
    "                        s = fb.get_score_from_std(std, i)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    hf = y - fb.W(y, i)\n",
    "                    dt = - 1.0 / N\n",
    "                    drift = (s/2 + hf) / dt\n",
    "                    drift = to_flattened_numpy(drift)\n",
    "                    return drift\n",
    "                x_N = fb.get_x_N([bsize, input_nc, resolution, resolution], N)\n",
    "                solution = solve_ivp(ode_func, (1, 1e-3), to_flattened_numpy(x_N),\n",
    "                                     rtol=1e-3, atol=1e-3, method=\"RK45\")\n",
    "                nfe = solution.nfev\n",
    "                solution = torch.tensor(solution.y[:, -1]).reshape(x_N.shape).to(device).type(torch.float32)\n",
    "                \n",
    "                save_image(solution, \"./solution.jpg\")\n",
    "                print(f\"nfe = {nfe}\")\n",
    "                raise NotImplementedError\n",
    "            for x_0, _ in test_loader:\n",
    "                x_0 = x_0.to(device)\n",
    "                # for v in range(0, 250, 20):\n",
    "                #     x_0[:, :, v, :] = 0\n",
    "                if opt.fromprior:\n",
    "                    i = np.array([N - 1] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    pred = fb.get_x_N(x_0.shape, i)\n",
    "                    print(f\"pred.std() = {pred.std()}\")\n",
    "                else:\n",
    "                    i = np.array([N-1] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    pred = fb.get_x_i(x_0, i)\n",
    "                preds = [pred]\n",
    "\n",
    "                for i in reversed(range(1, N)):\n",
    "                    i = np.array([i] * x_0.shape[0])\n",
    "                    i = torch.from_numpy(i).to(device)\n",
    "                    if opt.gtscore:\n",
    "                        s = fb.get_score_gt(pred, x_0, i)\n",
    "                    else:\n",
    "                        if opt.loss_type == \"sm_simple\":\n",
    "                            s = model(pred, i)\n",
    "                        elif opt.loss_type == \"eps_simple\":\n",
    "                            eps = model(pred, i)\n",
    "                            s = fb.get_score_from_eps(eps, i)\n",
    "                        elif opt.loss_type == \"sm_exact\":\n",
    "                            s = model(pred, i)\n",
    "                        elif opt.loss_type == \"std_matching\":\n",
    "                            std = model(pred, i)\n",
    "                            s = fb.get_score_from_std(std, i)\n",
    "                        else:\n",
    "                            raise NotImplementedError\n",
    "                    s = fb.U_I_minus_B_Ut(s, i)\n",
    "                    rms = lambda x: torch.sqrt(torch.mean(x ** 2))\n",
    "                    # print(f\"rms(s) * fb._beta_i(i) = {rms(s) * fb._beta_i(i)[0]}\")\n",
    "                    hf = pred - fb.W(pred, i)\n",
    "                    # Anderson theorem\n",
    "                    pred1 = pred + hf # unsharpening mask filtering\n",
    "                    pred2 = pred1 + s  # # denoising\n",
    "                    if i[0] > 2:\n",
    "                        pred = pred2 + fb.U_I_minus_B_sqrt_Ut(torch.randn_like(pred), i) # inject noise\n",
    "                    else:\n",
    "                        pred = pred2\n",
    "                    # print(f\"i = {i[0]}, rmse = {torch.sqrt(torch.mean(pred**2))}, mean = {torch.mean(pred)} std = {torch.std(pred)}\")\n",
    "                    # assert rms(pred) < 100\n",
    "                    if (i[0]) % (N // 10) == 0:\n",
    "                        img = pred[0]\n",
    "                        preds.append(pred)\n",
    "\n",
    "                preds.append(pred)\n",
    "                assert x_0.shape == pred.shape\n",
    "                # visualize\n",
    "                grid = torch.cat(preds, dim=3) # grid_sample.shape: (bsize, channel, H, W * 12)\n",
    "                # (batch_size, channel, H, W * 12) -> (channel, H * bsize, W * 12)\n",
    "                grid = grid.permute(1, 0, 2, 3).contiguous().view(grid.shape[1], -1, grid.shape[3])\n",
    "                # (bsize, channel, H, W) -> (channel, H, W * bsize)\n",
    "                gt = x_0.permute(1, 2, 0, 3).contiguous().view(x_0.shape[1], -1, x_0.shape[3] * x_0.shape[0])\n",
    "                if cnt <= 2:\n",
    "                    utils.tensor_imsave(gt, \"./\" + dir, f\"{step}_{cnt}_GT.jpg\")\n",
    "                    utils.tensor_imsave(grid, \"./\" + dir, f\"{step}_{cnt}_pred.jpg\")\n",
    "              \n",
    "                cnt += 1\n",
    "                loss += TF.l1_loss(x_0, pred) / 2\n",
    "\n",
    "                if cnt == 2:\n",
    "                    break\n",
    "        print(f\"step: {step} loss: {loss}\")\n",
    "        writer.add_scalar('loss_val', loss, meta_iter)\n",
    "        f = open('./' + str(dir) + '/log.txt', 'a')\n",
    "\n",
    "        f.write(f\"Step: {step} loss: {loss}\" + '\\n')\n",
    "\n",
    "        f.close()\n",
    "        model.train()\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "    if step % opt.save_every == 1:\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            torch.save(model.module.state_dict(), os.path.join(dir, f\"model_{step}.ckpt\"))\n",
    "        else:\n",
    "            torch.save(model.state_dict(), os.path.join(dir, f\"model_{step}.ckpt\"))\n",
    "        if opt.use_ema:\n",
    "            optimizer.swap_parameters_with_ema(store_params_in_ema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60fec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
